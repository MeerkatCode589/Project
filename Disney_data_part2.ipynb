{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeerkatCode589/Project/blob/main/Disney_data_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Dq4umb7Y5XhF",
        "outputId": "b6b3119b-f5c6-491a-fef7-cb0028f96701"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           username                                             tweets  \\\n",
              "0          @BabeBro  can we get a remake or any game where i can pl...   \n",
              "1        @A3predict  disney i am happy the little mermaid live acti...   \n",
              "2  @MartinScholes01  sounds like the latest woke disney remake of t...   \n",
              "3    @zoyajanelotts  thelittlemermaid the little mermaid tlm disney...   \n",
              "4   @AlwaysVeryCold     the latest disney remake of the little mermaid   \n",
              "\n",
              "                                               query            timestamp  \\\n",
              "0  https://twitter.com/search?f=live&q=little%20m...  29-06-2023 11:58:00   \n",
              "1  Little Mermaid since:2022-01-01 lang:en @Disne...  29-06-2023 11:11:00   \n",
              "2  https://twitter.com/search?f=live&q=little%20m...  29-06-2023 08:49:00   \n",
              "3  https://twitter.com/search?f=live&q=ariel%20ha...  29-06-2023 06:23:00   \n",
              "4  https://twitter.com/search?f=live&q=little%20m...  29-06-2023 05:28:00   \n",
              "\n",
              "                                    tokenized_tweets  \n",
              "0  get remake game play mermaid next gen littleme...  \n",
              "1  disney happy little mermaid live action well s...  \n",
              "2  sounds like latest woke disney remake little m...  \n",
              "3  thelittlemermaid little mermaid tlm disney nor...  \n",
              "4                latest disney remake little mermaid  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed7ae97b-66b1-4649-8c4f-3edad824c8da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>tweets</th>\n",
              "      <th>query</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>tokenized_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@BabeBro</td>\n",
              "      <td>can we get a remake or any game where i can pl...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>29-06-2023 11:58:00</td>\n",
              "      <td>get remake game play mermaid next gen littleme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@A3predict</td>\n",
              "      <td>disney i am happy the little mermaid live acti...</td>\n",
              "      <td>Little Mermaid since:2022-01-01 lang:en @Disne...</td>\n",
              "      <td>29-06-2023 11:11:00</td>\n",
              "      <td>disney happy little mermaid live action well s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MartinScholes01</td>\n",
              "      <td>sounds like the latest woke disney remake of t...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>29-06-2023 08:49:00</td>\n",
              "      <td>sounds like latest woke disney remake little m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@zoyajanelotts</td>\n",
              "      <td>thelittlemermaid the little mermaid tlm disney...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=ariel%20ha...</td>\n",
              "      <td>29-06-2023 06:23:00</td>\n",
              "      <td>thelittlemermaid little mermaid tlm disney nor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@AlwaysVeryCold</td>\n",
              "      <td>the latest disney remake of the little mermaid</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>29-06-2023 05:28:00</td>\n",
              "      <td>latest disney remake little mermaid</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed7ae97b-66b1-4649-8c4f-3edad824c8da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed7ae97b-66b1-4649-8c4f-3edad824c8da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed7ae97b-66b1-4649-8c4f-3edad824c8da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-41c4a7ec-9ec8-493c-a63f-fa83b9f71547\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41c4a7ec-9ec8-493c-a63f-fa83b9f71547')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-41c4a7ec-9ec8-493c-a63f-fa83b9f71547 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "disneycleaned_df = pd.read_csv(r'/content/disney19670_tweets_nocomma.csv') #load data\n",
        "\n",
        "disneycleaned_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert 'timestamp' to datetime and handle missing values\n",
        "disneycleaned_df['timestamp'] = pd.to_datetime(disneycleaned_df['timestamp'], errors='coerce')\n",
        "\n",
        "if disneycleaned_df['timestamp'].isnull().any():\n",
        "\n",
        "    disneycleaned_df['timestamp'].fillna(pd.Timestamp.now(), inplace=True)\n",
        "\n",
        "# Create the 'year' column by extracting the year from 'timestamp'\n",
        "disneycleaned_df['year'] = disneycleaned_df['timestamp'].dt.year\n",
        "\n",
        "# Drop time part and keep only the date in 'timestamp' column\n",
        "disneycleaned_df['timestamp'] = disneycleaned_df['timestamp'].dt.date\n"
      ],
      "metadata": {
        "id": "wuBB87ouhW-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "nvqUc2E66_dn",
        "outputId": "c6d59274-8cec-4732-afb9-303f34d50cf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           username                                             tweets  \\\n",
              "0          @BabeBro  can we get a remake or any game where i can pl...   \n",
              "1        @A3predict  disney i am happy the little mermaid live acti...   \n",
              "2  @MartinScholes01  sounds like the latest woke disney remake of t...   \n",
              "3    @zoyajanelotts  thelittlemermaid the little mermaid tlm disney...   \n",
              "4   @AlwaysVeryCold     the latest disney remake of the little mermaid   \n",
              "\n",
              "                                               query   timestamp  \\\n",
              "0  https://twitter.com/search?f=live&q=little%20m...  2023-06-29   \n",
              "1  Little Mermaid since:2022-01-01 lang:en @Disne...  2023-06-29   \n",
              "2  https://twitter.com/search?f=live&q=little%20m...  2023-06-29   \n",
              "3  https://twitter.com/search?f=live&q=ariel%20ha...  2023-06-29   \n",
              "4  https://twitter.com/search?f=live&q=little%20m...  2023-06-29   \n",
              "\n",
              "                                    tokenized_tweets  year  \n",
              "0  get remake game play mermaid next gen littleme...  2023  \n",
              "1  disney happy little mermaid live action well s...  2023  \n",
              "2  sounds like latest woke disney remake little m...  2023  \n",
              "3  thelittlemermaid little mermaid tlm disney nor...  2023  \n",
              "4                latest disney remake little mermaid  2023  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c7ed758-5c24-465b-9efb-d41daf08f503\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>tweets</th>\n",
              "      <th>query</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>tokenized_tweets</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@BabeBro</td>\n",
              "      <td>can we get a remake or any game where i can pl...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>get remake game play mermaid next gen littleme...</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@A3predict</td>\n",
              "      <td>disney i am happy the little mermaid live acti...</td>\n",
              "      <td>Little Mermaid since:2022-01-01 lang:en @Disne...</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>disney happy little mermaid live action well s...</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MartinScholes01</td>\n",
              "      <td>sounds like the latest woke disney remake of t...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>sounds like latest woke disney remake little m...</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@zoyajanelotts</td>\n",
              "      <td>thelittlemermaid the little mermaid tlm disney...</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=ariel%20ha...</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>thelittlemermaid little mermaid tlm disney nor...</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@AlwaysVeryCold</td>\n",
              "      <td>the latest disney remake of the little mermaid</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20m...</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>latest disney remake little mermaid</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c7ed758-5c24-465b-9efb-d41daf08f503')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c7ed758-5c24-465b-9efb-d41daf08f503 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c7ed758-5c24-465b-9efb-d41daf08f503');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e8418e3-6b53-447e-88e9-2b55d508bb3b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e8418e3-6b53-447e-88e9-2b55d508bb3b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e8418e3-6b53-447e-88e9-2b55d508bb3b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "disneycleaned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH7gaVWh8I7h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your DataFrame is named 'disneycleaned_df' and you have a 'timestamp' column\n",
        "\n",
        "# Extract the year and month from the 'timestamp' column\n",
        "disneycleaned_df['year'] = pd.DatetimeIndex(disneycleaned_df['timestamp']).year\n",
        "disneycleaned_df['month'] = pd.DatetimeIndex(disneycleaned_df['timestamp']).month\n",
        "\n",
        "# Filter the DataFrame for tweets in the first 6 months of 2023\n",
        "tweets_2023_h1 = disneycleaned_df[(disneycleaned_df['year'] == 2023) & (disneycleaned_df['month'] <= 6)]\n",
        "\n",
        "# Count the number of tweets per month in the first half of 2023\n",
        "monthly_tweet_counts = tweets_2023_h1.groupby(['year', 'month']).size().reset_index(name='tweet_count')\n",
        "\n",
        "# Create a line plot with improved aesthetics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_tweet_counts['month'], monthly_tweet_counts['tweet_count'], marker='o', linestyle='-', color='skyblue', linewidth=2, markersize=8, markerfacecolor='white', markeredgecolor='skyblue')\n",
        "plt.fill_between(monthly_tweet_counts['month'], monthly_tweet_counts['tweet_count'], color='skyblue', alpha=0.2)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.title('Number of Tweets in the First 6 Months of 2023')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, 7), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])  # Setting x-axis ticks and labels\n",
        "plt.ylim(0, max(monthly_tweet_counts['tweet_count']) * 1.1)  # Adjusting y-axis limits for better visualization\n",
        "\n",
        "# Save the plot as a JPEG file\n",
        "plt.savefig('tweets_2023_h2.jpg', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrKsgNSx0MeE"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# convert string of tokens into tokens list\n",
        "import re\n",
        "disneycleaned_df['tokenized_tweets'] = disneycleaned_df.tokenized_tweets.apply(lambda x: re.split('\\s', x))\n",
        "disneycleaned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwmMeXbmDVf5"
      },
      "outputs": [],
      "source": [
        "import gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBs5kV4iDwRA"
      },
      "outputs": [],
      "source": [
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vywzF3mUD1DE"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_soKy_gDsN2",
        "outputId": "c99f7036-2ad5-4741-cba2-b2186c9ceec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YIwkgCPETP-",
        "outputId": "bf370d92-fee4-46d0-f77b-8a7007a357da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPyM-TM\n",
            "  Downloading GPyM_TM-3.0.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: GPyM-TM\n",
            "Successfully installed GPyM-TM-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install GPyM-TM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ionBoYfQJ9kb",
        "outputId": "675c6bc9-6e01-4c7d-f305-95377354cddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/rwalk/gsdmm.git\n",
            "  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-87idz35v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-87idz35v\n",
            "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.22.4)\n",
            "Building wheels for collected packages: gsdmm\n",
            "  Building wheel for gsdmm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gsdmm: filename=gsdmm-0.1-py3-none-any.whl size=4585 sha256=f6d3ec63df4957f2fef339c92517e56b7e002425f2cc5279cf7a4dd3be5802e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vqjlztzn/wheels/da/d3/6e/a612d7cff0fcfb6470b8c113fc04931ecffb466ac19b9c5f3c\n",
            "Successfully built gsdmm\n",
            "Installing collected packages: gsdmm\n",
            "Successfully installed gsdmm-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/rwalk/gsdmm.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SYoTfYc3e5a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten the 'tokens' column to create a list of all tokens in the DataFrame\n",
        "all_tokens = [token for tokens_list in disneycleaned_df['tokenized_tweets'] for token in tokens_list]\n",
        "\n",
        "# Calculate the frequency distribution of each word in the tokenized tweets column\n",
        "word_freq = pd.Series(all_tokens).value_counts().reset_index()\n",
        "word_freq.columns = ['word', 'frequency']\n",
        "\n",
        "# Calculate the cumulative distribution\n",
        "word_freq['cumulative_distinct_words'] = word_freq['frequency'].cumsum()\n",
        "word_freq['distinct_tokens'] = word_freq.index + 1\n",
        "word_freq['corpus_percent'] = (word_freq['cumulative_distinct_words'] / len(all_tokens)) * 100\n",
        "word_freq['distinct_words_percent'] = (word_freq['distinct_tokens'] / len(set(all_tokens))) * 100\n",
        "\n",
        "# Calculate the percentage of distinct words and corpus covered by the most and least frequent words\n",
        "most_freq_word_data = word_freq.iloc[31]\n",
        "least_freq_word_data = word_freq.iloc[-1]\n",
        "\n",
        "# Print the results\n",
        "print(\"Most frequent {} unique words, which are {:.1f}% of the total distinct words, are contributing to {:.0f}% of the total corpus.\".format(\n",
        "    most_freq_word_data['distinct_tokens'], most_freq_word_data['distinct_words_percent'], most_freq_word_data['corpus_percent']\n",
        "))\n",
        "print(\"Least frequent {} unique words, which are {:.1f}% of the total distinct words, are contributing only to the bottom {:.0f}% of the total corpus.\".format(\n",
        "    least_freq_word_data['distinct_tokens'], least_freq_word_data['distinct_words_percent'], (1 - least_freq_word_data['corpus_percent']) * 100\n",
        "))\n",
        "print(\"--------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2BpLQ7AeavM",
        "outputId": "43e1364a-eff3-4d6f-9a08-35bbda711b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweets'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    from nltk import FreqDist\n",
        "\n",
        "    # Calculate the frequency distribution of words\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "\n",
        "result_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "ySnnZ8sw3pvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweets'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    from nltk import FreqDist\n",
        "\n",
        "    # Calculate the frequency distribution of words\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word in most_common_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "percentile_to_keep = 75\n",
        "filtered_tokens = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "filtered_df = pd.DataFrame({'text': [\" \".join(filtered_tokens)]})\n",
        "\n"
      ],
      "metadata": {
        "id": "b4QV0TA35a6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "filtered_df['tokens'] = filtered_df['text'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "filtered_tokens = [word for sublist in filtered_df['tokens'] for word in sublist]\n",
        "\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Calculate the frequency distribution of words\n",
        "freq_dist_filtered = FreqDist(filtered_tokens)\n",
        "\n",
        "\n",
        "print(freq_dist_filtered.most_common(10))\n",
        "\n",
        "filtered_freq_df = pd.DataFrame(freq_dist_filtered.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "print(filtered_freq_df)\n"
      ],
      "metadata": {
        "id": "W2eqJR5H6BoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 20 most common words\n",
        "num_words_to_plot = 20\n",
        "top_words = filtered_freq_df.head(num_words_to_plot)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_words['Word'], top_words['Frequency'])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Common Words (75% Most Frequent)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5DBXr3Vz6J5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and tokenize the text in your 'disneycleaned_df'\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweets'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['filtered_text'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n",
        "\n"
      ],
      "metadata": {
        "id": "uK2A9Qs_6z0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered_text'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    from nltk import FreqDist\n",
        "\n",
        "    # Calculate the frequency distribution of words\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "result_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "cLfjk95Q6YgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered_text'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['filtered'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n",
        "\n"
      ],
      "metadata": {
        "id": "kKLqC_4N9XDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "filterresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(filterresult_df)\n"
      ],
      "metadata": {
        "id": "RPQxaTad-p_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "# Calculate the frequency distribution of words in the 'filtered_text' column\n",
        "freq_dist_filtered = FreqDist(flattened_tokens)\n",
        "\n",
        "most_common_words = freq_dist_filtered.most_common(25)\n",
        "print(\"25 Most Frequent Words:\")\n",
        "print(most_common_words)\n",
        "\n",
        "total_distinct_words = len(freq_dist_filtered)\n",
        "least_common_words = freq_dist_filtered.most_common()[::-1][:25]\n",
        "print(\"\\n25 Least Frequent Words:\")\n",
        "print(least_common_words)\n"
      ],
      "metadata": {
        "id": "Up9WHQFPAQWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "\n",
        "disneycleaned_df['shorttext'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n",
        "\n"
      ],
      "metadata": {
        "id": "x4UfZ6DOAvTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['shorttext'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "ne8lWN0MCOUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['shorttext'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n"
      ],
      "metadata": {
        "id": "ipuPZyBRC34H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "PnEHBoJvWSsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "Qb1FQEiPFAWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety2'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n",
        "\n"
      ],
      "metadata": {
        "id": "G48SQ35MGwgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety2'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "fHDC8yO5Q6TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety2'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety3'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n"
      ],
      "metadata": {
        "id": "I3ogL5ihX1p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety3'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "Tdh0vI13YJs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety3'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Get the number of words required to cover the given percentile of the total distinct words\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    # Get the most common words\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety4'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words])\n"
      ],
      "metadata": {
        "id": "YeOidhnTfndp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety4'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n",
        "print(shortresult_df)"
      ],
      "metadata": {
        "id": "XeNwpPtOY2e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety4'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 75\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety5'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n"
      ],
      "metadata": {
        "id": "c_Mn_0ZfYl_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety5'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n"
      ],
      "metadata": {
        "id": "7moOtgd3ZEfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety5'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens, percentile_to_keep):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    num_words_to_keep = int(total_distinct_words * percentile_to_keep / 100)\n",
        "\n",
        "    most_common_words = [word for word, _ in freq_dist.most_common(num_words_to_keep)]\n",
        "\n",
        "    return most_common_words\n",
        "\n",
        "percentile_to_keep = 90\n",
        "most_common_words = dynamic_stop_word_analyzer(flattened_tokens, percentile_to_keep)\n",
        "\n",
        "disneycleaned_df['tweety6'] = disneycleaned_df['tokens'].apply(lambda tokens: \" \".join([word for word in tokens if word in most_common_words]))\n",
        "\n"
      ],
      "metadata": {
        "id": "S77WJix5ZcQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety6'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Flatten the list of lists to a list of words\n",
        "flattened_tokens = [word for sublist in disneycleaned_df['tokens'] for word in sublist]\n",
        "\n",
        "def dynamic_stop_word_analyzer(tokens):\n",
        "\n",
        "    freq_dist = FreqDist(tokens)\n",
        "\n",
        "    # Compute total distinct words and total corpus size\n",
        "    total_distinct_words = len(freq_dist)\n",
        "    total_corpus_size = len(tokens)\n",
        "\n",
        "    # Create a DataFrame to store the statistics\n",
        "    statistics_df = pd.DataFrame(columns=['num_of_words', 'dist_words%', 'corpus%'])\n",
        "\n",
        "    # Define the percentiles for computing the statistics\n",
        "    percentiles = [75, 80, 85, 90, 95, 100]\n",
        "\n",
        "    for percentile in percentiles:\n",
        "        num_words = int(total_distinct_words * percentile / 100)\n",
        "        words_freq_list = freq_dist.most_common(num_words)\n",
        "        words_corpus_coverage = sum([count for _, count in words_freq_list]) / total_corpus_size\n",
        "        dist_words_coverage = num_words / total_distinct_words * 100\n",
        "\n",
        "        statistics_df = statistics_df.append({\n",
        "            'num_of_words': num_words,\n",
        "            'dist_words%': dist_words_coverage,\n",
        "            'corpus%': words_corpus_coverage * 100\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    return statistics_df\n",
        "\n",
        "shortresult_df = dynamic_stop_word_analyzer(flattened_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "n17tjuliZtl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety6'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Step 2: Define the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 3: Filter out stopwords and single letters from the tokens\n",
        "def filter_stopwords_and_single_letters(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    return filtered_tokens\n",
        "\n",
        "disneycleaned_df['filtered_tokens'] = disneycleaned_df['tokens'].apply(filter_stopwords_and_single_letters)\n",
        "\n",
        "\n",
        "disneycleaned_df.head()\n"
      ],
      "metadata": {
        "id": "v27wTCpwb_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['tweety6'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def filter_tokens(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    filtered_tokens = list(set(filtered_tokens))  # Convert to set and back to list to remove duplicates\n",
        "    return filtered_tokens\n",
        "\n",
        "disneycleaned_df['filtered_tokens'] = disneycleaned_df['tokens'].apply(filter_tokens)\n",
        "\n",
        "disneycleaned_df.head()\n"
      ],
      "metadata": {
        "id": "dsRYSEC6cnqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "columns_to_delete = ['filtered_text', 'tokens', 'shorttext', 'filtered', 'filtered_text', 'tweety', 'tweety2', 'tweety3', 'tweety4', 'tweety5']\n",
        "disneycleaned_df.drop(columns=columns_to_delete, inplace=True)\n",
        "\n",
        "new_netflixcleaned_df = disneycleaned_df.drop(columns=columns_to_delete)\n"
      ],
      "metadata": {
        "id": "gjFKEFsOhO-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the display module from IPython\n",
        "from IPython.display import display\n",
        "\n",
        "# Set the maximum width for displaying columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Now display the entire DataFrame with full words\n",
        "display(disneycleaned_df)\n"
      ],
      "metadata": {
        "id": "aRtvLQ7wgubj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the 'tweety6' column to 'filtered_tweets'\n",
        "disneycleaned_df.rename(columns={'tweety6': 'filtered_tweets'}, inplace=True)"
      ],
      "metadata": {
        "id": "giTL6BwijUYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disneycleaned_df.head()"
      ],
      "metadata": {
        "id": "UxBkmECXK3SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Step 1: Tokenize the text in the 'filtered_tweets' column\n",
        "disneycleaned_df['tokens'] = disneycleaned_df['filtered_tweets'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Step 2: Define a custom list of stopwords\n",
        "custom_stopwords = set(['would', 'disneyplus', 'let', 'ebay', 'sims', 'comicsgate', 'ang', 'ben', 'pc', 'gabriella', 'npr', 'nkd', 'dari', 'playlist', 'financial', 'wave', 'fm', 'fc', 'yet', 'get', 'live', 'watch', 'count', 'heels', 'dx', 'september', 'lnkd', 'rawstory', 'theroot', 'faced', 'ik', 'really', 'may', 'lt', 'com', 'see', 'film', 'a', 'action', 'little', 'probaly', 'also'])\n",
        "custom_stopwords = set(word.lower() for word in custom_stopwords)  # Convert to lowercase\n",
        "\n",
        "# Step 3: Filter out stopwords from the tokens\n",
        "def filter_stopwords(tokens):\n",
        "    filtered_tokens2 = [token for token in tokens if token.lower() not in stopwords.words('english') and token.lower() not in custom_stopwords and len(token) > 1]\n",
        "    return filtered_tokens2\n",
        "\n",
        "disneycleaned_df['filtered_tokens2'] = disneycleaned_df['tokens'].apply(filter_stopwords)\n",
        "\n",
        "# Step 4: Create a frequency distribution of words\n",
        "all_tokens = [word for sublist in disneycleaned_df['filtered_tokens2'] for word in sublist]\n",
        "freq_dist = FreqDist(all_tokens)\n",
        "\n",
        "# Step 5: Get the most frequent words (e.g., top 25)\n",
        "most_frequent_words = freq_dist.most_common(40)\n",
        "print(\"Most frequent words:\")\n",
        "print(most_frequent_words)\n",
        "\n",
        "# Step 6: Get the least frequent words (e.g., bottom 25)\n",
        "least_frequent_words = freq_dist.most_common()[-25:][::-1]\n",
        "print(\"Least frequent words:\")\n",
        "print(least_frequent_words)\n"
      ],
      "metadata": {
        "id": "FIQFEKiRFTqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#: Filter out stopwords from the tokens and remove single letters and duplicates\n",
        "def filter_tokens(tokens):\n",
        "    filtered_tokens = [token for token in tokens if\n",
        "                       token.lower() not in stopwords.words('english') and\n",
        "                       token.lower() not in custom_stopwords and\n",
        "                       len(token) > 1]  # Remove stopwords and single letters\n",
        "\n",
        "    # Remove duplicates\n",
        "    filtered_tokens = list(dict.fromkeys(filtered_tokens))\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "disneycleaned_df['filtered_tokens2'] = disneycleaned_df['tokens'].apply(filter_tokens)\n",
        "\n",
        "# Create a frequency distribution of words\n",
        "all_tokens = [word for sublist in disneycleaned_df['filtered_tokens2'] for word in sublist]\n",
        "freq_dist = FreqDist(all_tokens)\n"
      ],
      "metadata": {
        "id": "GJv1g3EPMbQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Drop the 'tokens' column\n",
        "disneycleaned_df.drop('tokens', axis=1, inplace=True)\n",
        "\n",
        "disneycleaned_df.drop('filtered_tokens', axis=1, inplace=True)\n",
        "\n",
        "disneycleaned_df.drop('tokenized_tweets', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "etb1zLhWOdLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disneycleaned_df.rename(columns={'filtered_tokens2': 'filtered_tokens'}, inplace=True)"
      ],
      "metadata": {
        "id": "w8UQ32dQO6Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA4dJwoPKQm9"
      },
      "outputs": [],
      "source": [
        "from gsdmm import MovieGroupProcess\n",
        "mgp = MovieGroupProcess(K=7, alpha=0.1, beta=0.01, n_iters=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2ADA9XKOYkr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Specify the file path and name to save the model\n",
        "file_path = 'dumps/trained_models/7clusters.model'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Open the file in write mode and save the model using pickle\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(mgp, file)\n",
        "\n",
        "# Close the file\n",
        "file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErrqgmmzDJtg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train STTM model\n",
        "#    K = number of potential topics\n",
        "#    alpha = controls completeness\n",
        "#    beta =  controls homogeneity\n",
        "#    n_iters = number of iterations\n",
        "vocab = set(x for doc in docs for x in doc)\n",
        "n_terms = len(vocab)\n",
        "y = mgp.fit(docs, n_terms)\n",
        "\n",
        "# Save model\n",
        "with open('dumps/trained_models/7clusters.model', 'wb') as f:\n",
        "    pickle.dump(mgp, f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By0HCtPMO3U3"
      },
      "outputs": [],
      "source": [
        "# load in trained model\n",
        "filehandler = open('dumps/trained_models/7clusters.model', 'rb')\n",
        "mgp = pickle.load(filehandler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2c5DrD_O30-"
      },
      "outputs": [],
      "source": [
        "# define helper functions\n",
        "def top_words(cluster_word_distribution, top_cluster, values):\n",
        "    '''prints the top words in each cluster'''\n",
        "    for cluster in top_cluster:\n",
        "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
        "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
        "        print('         ')\n",
        "\n",
        "def cluster_importance(mgp):\n",
        "    '''returns a word-topic matrix[phi] where each value represents\n",
        "    the word importance for that particular cluster;\n",
        "    phi[i][w] would be the importance of word w in topic i.\n",
        "    '''\n",
        "    n_z_w = mgp.cluster_word_distribution\n",
        "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
        "    phi = [{} for i in range(K)]\n",
        "    for z in range(K):\n",
        "        for w in n_z_w[z]:\n",
        "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
        "    return phi\n",
        "\n",
        "def topic_allocation(df, docs, mgp, topic_dict):\n",
        "    '''allocates all topics to each document in original dataframe,\n",
        "    adding two columns for cluster number and cluster description'''\n",
        "    topic_allocations = []\n",
        "    for doc in tqdm(docs):\n",
        "        topic_label, score = mgp.choose_best_label(doc)\n",
        "        topic_allocations.append(topic_label)\n",
        "\n",
        "    df['cluster'] = topic_allocations\n",
        "\n",
        "    df['topic_name'] = df.cluster.apply(lambda x: get_topic_name(x, topic_dict))\n",
        "    print('Complete. Number of documents with topic allocated: {}'.format(len(df)))\n",
        "\n",
        "def get_topic_name(doc, topic_dict):\n",
        "    '''returns the topic name string value from a dictionary of topics'''\n",
        "    topic_desc = topic_dict[doc]\n",
        "    return topic_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1d-I2nPw4u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrLz7F6-PZAF"
      },
      "outputs": [],
      "source": [
        "doc_count = np.array(mgp.cluster_doc_count)\n",
        "print('Number of documents per topic :', doc_count)\n",
        "print('*'*20)\n",
        "\n",
        "# topics sorted by the number of documents they are allocated to\n",
        "top_index = doc_count.argsort()[-10:][::-1]\n",
        "print('Most important clusters (by number of docs inside):', top_index)\n",
        "print('*'*20)\n",
        "\n",
        "# show the top 7 words in term frequency for each cluster\n",
        "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
        "top_words(mgp.cluster_word_distribution, topic_indices, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQjY81MQF-PA"
      },
      "outputs": [],
      "source": [
        "unwanted_words = ['projectsegfau', 'mermaid', 'nitter', 'bit', 'star', 'premiere', 'watch', 'princess', 'coming', 'news', 'remakes', 'weekend', 'movies', 'little', 'part', 'film', 'disneystudios', 'halle', 'ly', 'bailey', 'theaters', 'trailer', 'live', 'invidious', 'via', 'see', 'lin', 'sea', 'de', 'littlemermaid', 'youtube', 'via' 'ly', 'dlvr', 'first', 'lin', 'may', 'disney', 'action', 'lt', 'com', 'movie', 'said', 'like', 'let', 'would', 'alone']\n",
        "def remove_unwanted_words(tokens):\n",
        "    return [token for token in tokens if token not in unwanted_words]\n",
        "\n",
        "# Apply the function to the tokenized_tweets column\n",
        "disneycleaned_df['filtered_tokens'] = disneycleaned_df['filtered_tokens'].apply(remove_unwanted_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMBL7MnuRe4B"
      },
      "outputs": [],
      "source": [
        "def top_words(cluster_word_distribution, top_cluster, values, unwanted_words):\n",
        "    '''prints the top words in each cluster, excluding unwanted words'''\n",
        "    for cluster in top_cluster:\n",
        "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)\n",
        "        filtered_words = [(word, freq) for word, freq in sort_dicts if word not in unwanted_words][:values]\n",
        "        print('Cluster %s : %s' % (cluster, filtered_words))\n",
        "        print('         ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ov_Oj0TRcgF",
        "outputId": "486affb9-e16d-48fc-97f7-786fff890140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0 : [('remake', 1530), ('ariel', 1022), ('new', 308), ('cast', 250), ('look', 204), ('thelittlemermaid', 168), ('manuel', 159)]\n",
            "         \n",
            "Cluster 1 : [('remake', 1308), ('eric', 419), ('prince', 392), ('play', 283), ('ariel', 240), ('harry', 213), ('cast', 208)]\n",
            "         \n",
            "Cluster 2 : [('remake', 4286), ('ariel', 743), ('going', 690), ('new', 497), ('one', 487), ('black', 438), ('good', 426)]\n",
            "         \n",
            "Cluster 3 : [('remake', 529), ('ariel', 195), ('new', 193), ('thelittlemermaid', 160), ('woke', 152), ('race', 151), ('hallebailey', 98)]\n",
            "         \n",
            "Cluster 4 : [('ariel', 1995), ('black', 1706), ('race', 1616), ('remake', 1445), ('white', 1052), ('woke', 1040), ('people', 906)]\n",
            "         \n",
            "Cluster 5 : [('ariel', 1597), ('hallebailey', 998), ('remake', 965), ('thelittlemermaid', 930), ('best', 451), ('new', 302), ('one', 293)]\n",
            "         \n",
            "Cluster 6 : [('ariel', 855), ('thelittlemermaid', 780), ('remake', 770), ('hallebailey', 702), ('world', 373), ('new', 311), ('ursula', 164)]\n",
            "         \n"
          ]
        }
      ],
      "source": [
        "top_words(mgp.cluster_word_distribution, topic_indices, 7, unwanted_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydo0e227dlj3"
      },
      "outputs": [],
      "source": [
        "phi = cluster_importance(mgp) # initialize phi matrix\n",
        "\n",
        "# 'woke' term importance for cluster 2 and 3\n",
        "print(phi[3]['woke'])\n",
        "print(phi[4]['woke'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHi_bbFtkuI4"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY8Vu23kelbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8492f2-8de9-41a0-ec4b-33edd0f62aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 19670/19670 [00:06<00:00, 2851.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete. Number of documents with topic allocated: 19670\n"
          ]
        }
      ],
      "source": [
        "# Define dictionary topics in the same sequential order\n",
        "# as resulting clusters from the gsdmm model\n",
        "topic_dict = {}\n",
        "topic_names = ['motivation',\n",
        "               'realistic plot',\n",
        "               'inclusion',\n",
        "               'fit',\n",
        "               'social context independency',\n",
        "               'conformity',\n",
        "               'historical value']\n",
        "\n",
        "for i, (topic_num, topic_name) in enumerate(zip(topic_indices, topic_names)):\n",
        "    topic_dict[topic_num] = topic_name\n",
        "\n",
        "topic_allocation(disneycleaned_df, docs, mgp, topic_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djG8ETuCEqHK"
      },
      "outputs": [],
      "source": [
        "def top_words_dict(cluster_word_distribution, top_cluster, n_words, unwanted_words):\n",
        "    '''returns a dictionary of the top n words and the number of docs they are in;\n",
        "    cluster numbers are the keys and a tuple of (word, word count) are the values'''\n",
        "    top_words_dict = {}\n",
        "    for cluster in top_cluster:\n",
        "        top_words_list = []\n",
        "        for val in range(0, n_words):\n",
        "            top_n_word = sorted(\n",
        "                [(word, count) for word, count in mgp.cluster_word_distribution[cluster].items() if word not in unwanted_words],\n",
        "                key=lambda item: item[1], reverse=True)[:n_words][val]\n",
        "            top_words_list.append(top_n_word)\n",
        "        top_words_dict[cluster] = top_words_list\n",
        "\n",
        "    return top_words_dict\n",
        "\n",
        "\n",
        "def get_word_counts_dict(top_words_nclusters, unwanted_words):\n",
        "    '''returns a dictionary that counts the number of times a word\n",
        "    appears only in the top n words list across all the clusters;\n",
        "    words are the keys and a count of the word is the value'''\n",
        "    word_count_dict = {}\n",
        "    for key in top_words_nclusters:\n",
        "        words_score_list = []\n",
        "        for word in top_words_nclusters[key]:\n",
        "            if word[0] not in unwanted_words:\n",
        "                if word[0] in word_count_dict.keys():\n",
        "                    word_count_dict[word[0]] += 1\n",
        "                else:\n",
        "                    word_count_dict[word[0]] = 1\n",
        "    return word_count_dict\n",
        "\n",
        "\n",
        "def get_cluster_importance_dict(top_words_nclusters, phi, unwanted_words):\n",
        "    '''returns a dictionary that of all top words and their cluster\n",
        "    importance value for each cluster;\n",
        "    cluster numbers are the keys and a list of word\n",
        "    importance computed scores are the values'''\n",
        "    cluster_importance_dict = {}\n",
        "    for key in top_words_nclusters:\n",
        "        words_score_list = []\n",
        "        for word in top_words_nclusters[key]:\n",
        "            if word[0] not in unwanted_words:\n",
        "                importance_score = phi[key][word[0]]\n",
        "                words_score_list.append(importance_score)\n",
        "        cluster_importance_dict[key] = words_score_list\n",
        "    return cluster_importance_dict\n",
        "\n",
        "\n",
        "def get_doc_counts_dict(top_words_nclusters, unwanted_words):\n",
        "    '''returns a dictionary of only the doc counts of each top n word for each cluster;\n",
        "    cluster numbers are the keys and a list of doc counts are the values'''\n",
        "    doc_counts_dict = {}\n",
        "    for key in top_words_nclusters:\n",
        "        doc_counts_list = []\n",
        "        for word in top_words_nclusters[key]:\n",
        "            if word[0] not in unwanted_words:\n",
        "                num_docs = word[1]\n",
        "                doc_counts_list.append(num_docs)\n",
        "        doc_counts_dict[key] = doc_counts_list\n",
        "    return doc_counts_dict\n",
        "\n",
        "\n",
        "def get_word_frequency_dict(top_words_nclusters, word_counts, unwanted_words):\n",
        "    '''returns a dictionary of only the number of occurrences across all\n",
        "    clusters for each word in a particular cluster's top n words;\n",
        "    cluster numbers are the keys and a list of\n",
        "    word occurrences counts are the values'''\n",
        "    word_frequency_dict = {}\n",
        "    for key in top_words_nclusters:\n",
        "        words_count_list = []\n",
        "        for word in top_words_nclusters[key]:\n",
        "            if word[0] not in unwanted_words:\n",
        "                words_count_list.append(word_counts[word[0]])\n",
        "        word_frequency_dict[key] = words_count_list\n",
        "\n",
        "    return word_frequency_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS0J0KFLWeBf"
      },
      "outputs": [],
      "source": [
        "def top_words_dict(cluster_word_distribution, top_cluster, n_words, unwanted_words):\n",
        "    '''returns a dictionary of the top n words and the number of docs they are in;\n",
        "    cluster numbers are the keys and a tuple of (word, word count) are the values'''\n",
        "\n",
        "    unwanted_set = set(unwanted_words)\n",
        "\n",
        "    top_words_dict = {}\n",
        "    for cluster in top_cluster:\n",
        "        top_words_list = []\n",
        "\n",
        "        # Filter out unwanted words from the cluster_word_distribution\n",
        "        filtered_cluster_word_dist = {word: count for word, count in cluster_word_distribution[cluster].items()\n",
        "                                      if word not in unwanted_set}\n",
        "\n",
        "        # Get the top n words for the filtered cluster word distribution\n",
        "        top_n_words = sorted(filtered_cluster_word_dist.items(), key=lambda item: item[1], reverse=True)[:n_words]\n",
        "\n",
        "        for word, count in top_n_words:\n",
        "            top_words_list.append((word, count))\n",
        "\n",
        "        top_words_dict[cluster] = top_words_list\n",
        "\n",
        "    return top_words_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1GAMmUinD9z"
      },
      "outputs": [],
      "source": [
        "# declare any static variables needed\n",
        "nwords = 10\n",
        "nclusters = len(topic_names)\n",
        "phi = cluster_importance(mgp)\n",
        "\n",
        "# define and generate dictionaries that hold each topic number and its values\n",
        "top_words = top_words_dict(mgp.cluster_word_distribution, topic_indices, nwords, unwanted_words)\n",
        "word_count = get_word_counts_dict(top_words, unwanted_words)\n",
        "word_frequency = get_word_frequency_dict(top_words, word_count, unwanted_words)\n",
        "cluster_importance_dict = get_cluster_importance_dict(top_words, phi, unwanted_words)\n",
        "\n",
        "\n",
        "# add all values for each topic to a list of lists\n",
        "rows_list = []\n",
        "for cluster in range(0, nclusters):\n",
        "    topic_name = topic_names[cluster]\n",
        "    words = [x[0] for x in top_words[cluster]]\n",
        "    doc_counts = [x[1] for x in top_words[cluster]]\n",
        "\n",
        "    # create a list of values which represents a 'row' in our data frame\n",
        "    rows_list.append([int(cluster), topic_name, words, doc_counts,\n",
        "                      word_frequency[cluster], cluster_importance_dict[cluster]])\n",
        "\n",
        "topic_words_df = pd.DataFrame(data=rows_list,\n",
        "                              columns=['cluster', 'topic_name', 'top_words',\n",
        "                                       'doc_count', 'num_topic_occurrence', 'word_importance'])\n",
        "\n",
        "topic_words_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAWq1ivQtikx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "tweets_df = pd.read_csv(r'/content/sample_data/disney7topics_results.csv')\n",
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Deleting the first tweet (row with index 0)\n",
        "tweets_df.drop(index=0, inplace=True)\n",
        "\n",
        "# Reset the index after deleting the first row (optional)\n",
        "tweets_df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "Hkir6aSHOr71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V2quS5pgOxuP",
        "outputId": "9bdbeac9-616d-404f-b087-4a07b8523723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           username  \\\n",
              "0        @A3predict   \n",
              "1  @MartinScholes01   \n",
              "2    @zoyajanelotts   \n",
              "3   @AlwaysVeryCold   \n",
              "4  @MickeyL27321664   \n",
              "\n",
              "                                                                                                                                                                                                                                                                         tweets  \\\n",
              "0              disney i am happy the little mermaid live action did not do well why you don t spoil the legacy of some movies because of political correctness we know mermaid don t exist but we are all familiar with ariel being white not black yet you changed that to get   \n",
              "1                                                                                                                                                                                                               sounds like the latest woke disney remake of the little mermaid   \n",
              "2                                     thelittlemermaid the little mermaid tlm disney normal girl sza so is ctrl princess ariel prince eric eriel ericariel jalle arieleric halle bailey jonah hauer king edit fc fancam flounder sebastian scuttle vanessa ursula trend editing   \n",
              "3                                                                                                                                                                                                                                the latest disney remake of the little mermaid   \n",
              "4  this time i incarnated as the heroine of the live action version of the disney movie little mermaid ariel disney chose the african american singer halle bailey to play the role of the little mermaid because of her black complexion it s not easy to be released at early   \n",
              "\n",
              "                                                                                                                                             query  \\\n",
              "0                                                                                   Little Mermaid since:2022-01-01 lang:en @Disney Little Mermaid   \n",
              "1                    https://twitter.com/search?f=live&q=little%20mermaid%20remake&src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid   \n",
              "2  https://twitter.com/search?f=live&q=ariel%20halle%20bailey%20disney%20lang%3Aen&src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid   \n",
              "3                    https://twitter.com/search?f=live&q=little%20mermaid%20remake&src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid   \n",
              "4  https://twitter.com/search?f=live&q=ariel%20halle%20bailey%20disney%20lang%3Aen&src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid   \n",
              "\n",
              "    timestamp  year  \\\n",
              "0  2023-06-29  2023   \n",
              "1  2023-06-29  2023   \n",
              "2  2023-06-29  2023   \n",
              "3  2023-06-29  2023   \n",
              "4  2023-06-29  2023   \n",
              "\n",
              "                                                                                                                                                                                                                                  filtered_tweets  \\\n",
              "0           disney i am happy the little mermaid live action did not do well why you don t the legacy of some movies because of political we know mermaid don t exist but we are all with ariel being white not black yet you changed that to get   \n",
              "1                                                                                                                                                                                 sounds like the latest woke disney remake of the little mermaid   \n",
              "2                                                               thelittlemermaid the little mermaid tlm disney girl so is princess ariel prince eric halle bailey jonah hauer king edit fc fancam flounder sebastian scuttle vanessa ursula trend   \n",
              "3                                                                                                                                                                                                  the latest disney remake of the little mermaid   \n",
              "4  this time i as the of the live action version of the disney movie little mermaid ariel disney chose the african american singer halle bailey to play the role of the little mermaid because of her black it s not easy to be released at early   \n",
              "\n",
              "                                                                                                                                                                filtered_tokens  \\\n",
              "0                                                                               ['happy', 'well', 'legacy', 'political', 'know', 'exist', 'ariel', 'white', 'black', 'changed']   \n",
              "1                                                                                                                                        ['sounds', 'latest', 'woke', 'remake']   \n",
              "2  ['thelittlemermaid', 'tlm', 'girl', 'ariel', 'prince', 'eric', 'jonah', 'hauer', 'king', 'edit', 'fancam', 'flounder', 'sebastian', 'scuttle', 'vanessa', 'ursula', 'trend']   \n",
              "3                                                                                                                                                          ['latest', 'remake']   \n",
              "4                                                  ['time', 'version', 'ariel', 'chose', 'african', 'american', 'singer', 'play', 'role', 'black', 'easy', 'released', 'early']   \n",
              "\n",
              "   cluster                   topic_name  \n",
              "0        4  social context independency  \n",
              "1        2                    inclusion  \n",
              "2        5                   conformity  \n",
              "3        0                   motivation  \n",
              "4        4  social context independency  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-2e557948-8786-441b-8da9-b11006da7440\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>tweets</th>\n",
              "      <th>query</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>year</th>\n",
              "      <th>filtered_tweets</th>\n",
              "      <th>filtered_tokens</th>\n",
              "      <th>cluster</th>\n",
              "      <th>topic_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@A3predict</td>\n",
              "      <td>disney i am happy the little mermaid live action did not do well why you don t spoil the legacy of some movies because of political correctness we know mermaid don t exist but we are all familiar with ariel being white not black yet you changed that to get</td>\n",
              "      <td>Little Mermaid since:2022-01-01 lang:en @Disney Little Mermaid</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>2023</td>\n",
              "      <td>disney i am happy the little mermaid live action did not do well why you don t the legacy of some movies because of political we know mermaid don t exist but we are all with ariel being white not black yet you changed that to get</td>\n",
              "      <td>['happy', 'well', 'legacy', 'political', 'know', 'exist', 'ariel', 'white', 'black', 'changed']</td>\n",
              "      <td>4</td>\n",
              "      <td>social context independency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@MartinScholes01</td>\n",
              "      <td>sounds like the latest woke disney remake of the little mermaid</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20mermaid%20remake&amp;src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>2023</td>\n",
              "      <td>sounds like the latest woke disney remake of the little mermaid</td>\n",
              "      <td>['sounds', 'latest', 'woke', 'remake']</td>\n",
              "      <td>2</td>\n",
              "      <td>inclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@zoyajanelotts</td>\n",
              "      <td>thelittlemermaid the little mermaid tlm disney normal girl sza so is ctrl princess ariel prince eric eriel ericariel jalle arieleric halle bailey jonah hauer king edit fc fancam flounder sebastian scuttle vanessa ursula trend editing</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=ariel%20halle%20bailey%20disney%20lang%3Aen&amp;src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>2023</td>\n",
              "      <td>thelittlemermaid the little mermaid tlm disney girl so is princess ariel prince eric halle bailey jonah hauer king edit fc fancam flounder sebastian scuttle vanessa ursula trend</td>\n",
              "      <td>['thelittlemermaid', 'tlm', 'girl', 'ariel', 'prince', 'eric', 'jonah', 'hauer', 'king', 'edit', 'fancam', 'flounder', 'sebastian', 'scuttle', 'vanessa', 'ursula', 'trend']</td>\n",
              "      <td>5</td>\n",
              "      <td>conformity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@AlwaysVeryCold</td>\n",
              "      <td>the latest disney remake of the little mermaid</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=little%20mermaid%20remake&amp;src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>2023</td>\n",
              "      <td>the latest disney remake of the little mermaid</td>\n",
              "      <td>['latest', 'remake']</td>\n",
              "      <td>0</td>\n",
              "      <td>motivation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@MickeyL27321664</td>\n",
              "      <td>this time i incarnated as the heroine of the live action version of the disney movie little mermaid ariel disney chose the african american singer halle bailey to play the role of the little mermaid because of her black complexion it s not easy to be released at early</td>\n",
              "      <td>https://twitter.com/search?f=live&amp;q=ariel%20halle%20bailey%20disney%20lang%3Aen&amp;src=typed_query since:2022-01-01 lang:en @Disney Little Mermaid</td>\n",
              "      <td>2023-06-29</td>\n",
              "      <td>2023</td>\n",
              "      <td>this time i as the of the live action version of the disney movie little mermaid ariel disney chose the african american singer halle bailey to play the role of the little mermaid because of her black it s not easy to be released at early</td>\n",
              "      <td>['time', 'version', 'ariel', 'chose', 'african', 'american', 'singer', 'play', 'role', 'black', 'easy', 'released', 'early']</td>\n",
              "      <td>4</td>\n",
              "      <td>social context independency</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e557948-8786-441b-8da9-b11006da7440')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-7583e740-d999-4b50-aed5-d091f0de11a3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7583e740-d999-4b50-aed5-d091f0de11a3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-7583e740-d999-4b50-aed5-d091f0de11a3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e557948-8786-441b-8da9-b11006da7440 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e557948-8786-441b-8da9-b11006da7440');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "L6JXM4vVuD64",
        "outputId": "b3cdb6d7-13db-4f87-96fa-acbfc125fd81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   cluster                   topic_name\n",
              "0        0                   motivation\n",
              "1        1               realistic plot\n",
              "2        2                    inclusion\n",
              "3        3                          fit\n",
              "4        4  social context independency\n",
              "5        5                   conformity\n",
              "6        6             historical value"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-7d980f7c-3cea-4b0c-ba7c-42ae64796667\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "      <th>topic_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>motivation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>realistic plot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>inclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>fit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>social context independency</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>conformity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>historical value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d980f7c-3cea-4b0c-ba7c-42ae64796667')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-fd799948-6820-4091-bf13-2fa0f9ad3644\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd799948-6820-4091-bf13-2fa0f9ad3644')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-fd799948-6820-4091-bf13-2fa0f9ad3644 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d980f7c-3cea-4b0c-ba7c-42ae64796667 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d980f7c-3cea-4b0c-ba7c-42ae64796667');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ],
      "source": [
        "# create data frame of unique cluster/topic names only\n",
        "topics_df = tweets_df[['cluster', 'topic_name']].drop_duplicates().sort_values(by='cluster')\n",
        "topics_df.reset_index(inplace=True, drop=True)\n",
        "topics_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "cyEgNxUga_He",
        "outputId": "bbdfcf18-8d72-492d-ebe8-cb2d292843a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: pyparsing, matplotlib\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.0\n",
            "    Uninstalling pyparsing-3.1.0:\n",
            "      Successfully uninstalled pyparsing-3.1.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed matplotlib-3.7.2 pyparsing-3.0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install psutil\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-yTr05veJ_T",
        "outputId": "15ea5ad3-0056-4506-81d7-c3de872d6766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U plotly\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "feUQwgxkejZh",
        "outputId": "affaf02a-2338-4f18-ab07-98952cd63d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.13.1)\n",
            "Collecting plotly\n",
            "  Downloading plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.1)\n",
            "Installing collected packages: plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.13.1\n",
            "    Uninstalling plotly-5.13.1:\n",
            "      Successfully uninstalled plotly-5.13.1\n",
            "Successfully installed plotly-5.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_plotly_utils",
                  "plotly"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvh7a3ZJxoK9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
        "\n",
        "# Now extract the year and month from the timestamp column\n",
        "tweets_df['year'] = tweets_df['timestamp'].dt.year\n",
        "tweets_df['month'] = tweets_df['timestamp'].dt.month\n",
        "\n",
        "# Now filter the data for the first 6 months in 2023\n",
        "filtered_tweets_df = tweets_df[(tweets_df['year'] == 2023) & (tweets_df['month'] <= 6)]\n",
        "\n",
        "# Now create the pivot table and heatmap for the filtered data\n",
        "year_month_topic_counts = pd.pivot_table(data=filtered_tweets_df,\n",
        "                                        values='tweets',\n",
        "                                        index='month',\n",
        "                                        columns='cluster',\n",
        "                                        aggfunc='count',\n",
        "                                        fill_value=0)\n",
        "\n",
        "n_topics = len(year_month_topic_counts.columns)\n",
        "year_month_topic_counts.columns = ['Topic {}'.format(i) for i in range(n_topics)]\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(year_month_topic_counts, cmap='YlGnBu', annot=True, fmt='d', linewidths=0.5)\n",
        "plt.title('Tweet Counts by Topics for the First 6 Months of 2023')\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Months')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
        "\n",
        "# Now extract the year and month from the timestamp column\n",
        "tweets_df['year'] = tweets_df['timestamp'].dt.year\n",
        "tweets_df['month'] = tweets_df['timestamp'].dt.month\n",
        "\n",
        "# Now filter the data for the first 6 months in 2023\n",
        "filtered_tweets_df = tweets_df[(tweets_df['year'] == 2023) & (tweets_df['month'] <= 6)].copy()\n",
        "\n",
        "# Map the month numbers to the first 3 letters of the month names and set the order\n",
        "month_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun'}\n",
        "month_order = [month_map[i] for i in range(1, 7)]\n",
        "filtered_tweets_df['month'] = filtered_tweets_df['month'].map(month_map)\n",
        "filtered_tweets_df['month'] = pd.Categorical(filtered_tweets_df['month'], categories=month_order, ordered=True)\n",
        "\n",
        "# Map the cluster numbers to the corresponding topic names using topic_dict\n",
        "topic_dict = {0: 'motivation', 1: 'realistic plot', 2: 'inclusion', 3: 'fit',\n",
        "              4: 'social context independency', 5: 'conformity', 6: 'historical value'}\n",
        "filtered_tweets_df['cluster'] = filtered_tweets_df['cluster'].map(topic_dict)\n",
        "\n",
        "# create the pivot table and heatmap for the filtered data\n",
        "year_month_topic_counts = pd.pivot_table(data=filtered_tweets_df,\n",
        "                                        values='tweets',\n",
        "                                        index='month',\n",
        "                                        columns='cluster',\n",
        "                                        aggfunc='count',\n",
        "                                        fill_value=0)\n",
        "\n",
        "# Sort the columns of the pivot table according to the order of the clusters\n",
        "topic_order = ['motivation', 'realistic plot', 'inclusion', 'fit',\n",
        "               'social context independency', 'conformity', 'historical value']\n",
        "year_month_topic_counts = year_month_topic_counts[topic_order]\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(14, 8))  # Adjust the figure size to accommodate longer x-axis labels\n",
        "sns.heatmap(year_month_topic_counts, cmap='YlGnBu', annot=True, fmt='d', linewidths=0.5)\n",
        "plt.title('Tweet Counts by Topics for the First 6 Months of 2023')\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Months')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_RjQrE6eSUzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jukgpZMEzJgV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "topic_words_df = pd.read_pickle('/content/sample_data/disney7topics_words.pkl')\n",
        "topic_words_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter()\n",
        "\n",
        "# create a list to specify which topics to display first by default\n",
        "trace_default_visibilities = []\n",
        "\n",
        "for cluster in topic_words_df.cluster.values:\n",
        "    if cluster == 0:\n",
        "        trace_default_visibilities.append(True)\n",
        "    else:\n",
        "        trace_default_visibilities.append('legendonly')\n",
        "\n",
        "    # add each trace (set of scatter points for each cluster)\n",
        "    fig.add_scatter(x=topic_words_df.iloc[cluster].doc_count[0:10],  # Changed [0:5] to [0:10]\n",
        "                    y=topic_words_df.iloc[cluster].word_importance[0:10],  # Changed [0:5] to [0:10]\n",
        "                    mode='markers+text',\n",
        "                    opacity=0.6,\n",
        "                    marker=dict(size=topic_words_df.iloc[cluster].num_topic_occurrence[0:10],  # Changed [0:5] to [0:10]\n",
        "                                sizemode='area',\n",
        "                                sizeref=2. * max(topic_words_df.iloc[cluster].num_topic_occurrence[0:10]) / (40.** 2),\n",
        "                                sizemin=4\n",
        "                               ),\n",
        "                    hovertext=topic_words_df.iloc[cluster].top_words[0:10],  # Changed [0:5] to [0:10]\n",
        "                    hoverlabel=dict(font=dict(color='#FFFFFF')),\n",
        "                    text=topic_words_df.iloc[cluster].top_words[0:10],  # Changed [0:5] to [0:10]\n",
        "                    textposition='bottom center',\n",
        "                    visible=trace_default_visibilities[cluster],\n",
        "                    name=topic_words_df.iloc[cluster].topic_name\n",
        "                   )\n",
        "\n",
        "    fig.update_layout(\n",
        "        margin=dict(t=15,\n",
        "                    b=0,\n",
        "                    l=20,\n",
        "                    r=0),\n",
        "        xaxis=dict(title='Tweet Count'),\n",
        "        yaxis=dict(title='Word Importance'),\n",
        "        legend=dict(itemsizing='constant',\n",
        "                    title=dict(text='   <b>Topic</b>')),\n",
        "    )\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "bNgtKpe9pFK2",
        "outputId": "bce4b431-31a8-465d-c7da-34ceee6f3632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"b89a06cf-ccce-4911-bddb-67d68418035e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b89a06cf-ccce-4911-bddb-67d68418035e\")) {                    Plotly.newPlot(                        \"b89a06cf-ccce-4911-bddb-67d68418035e\",                        [{\"hovertemplate\":\"<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"remake\",\"ariel\",\"new\",\"cast\",\"look\",\"thelittlemermaid\",\"manuel\",\"hallebailey\",\"miranda\",\"black\"],\"marker\":{\"size\":[7,7,6,2,1,4,1,4,1,3],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"motivation\",\"opacity\":0.6,\"text\":[\"remake\",\"ariel\",\"new\",\"cast\",\"look\",\"thelittlemermaid\",\"manuel\",\"hallebailey\",\"miranda\",\"black\"],\"textposition\":\"bottom center\",\"visible\":true,\"x\":[1530,1022,308,250,204,168,159,151,145,141],\"y\":[0.0387989249978572,0.02591675174479908,0.007810705085973292,0.00633990577755327,0.005173409774323598,0.004260499858752549,0.004032272379859788,0.0038294035097328877,0.003677251857137713,0.0035758174220742634],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"remake\",\"eric\",\"prince\",\"play\",\"ariel\",\"harry\",\"cast\",\"styles\",\"ursula\",\"wants\"],\"marker\":{\"size\":[7,1,1,2,7,1,2,1,2,1],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"realistic plot\",\"opacity\":0.6,\"text\":[\"remake\",\"eric\",\"prince\",\"play\",\"ariel\",\"harry\",\"cast\",\"styles\",\"ursula\",\"wants\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[1308,419,392,283,240,213,208,198,177,121],\"y\":[0.05476662811817778,0.017544028599015046,0.01641353345051404,0.011849682665824797,0.010049264466360232,0.008918769317859227,0.008709418364433116,0.00829071645758089,0.00741144245319122,0.005066711774818765],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"remake\",\"ariel\",\"going\",\"new\",\"one\",\"black\",\"good\",\"people\",\"make\",\"think\"],\"marker\":{\"size\":[7,7,1,6,2,3,1,2,1,1],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"inclusion\",\"opacity\":0.6,\"text\":[\"remake\",\"ariel\",\"going\",\"new\",\"one\",\"black\",\"good\",\"people\",\"make\",\"think\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[4286,743,690,497,487,438,426,420,413,360],\"y\":[0.045814522806407694,0.007942269987794938,0.007375736146590739,0.0053126978191867695,0.005205804641601071,0.004682028071431152,0.004553756258328314,0.004489620351776896,0.004414795127466908,0.003848261286262709],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"remake\",\"ariel\",\"new\",\"thelittlemermaid\",\"woke\",\"race\",\"hallebailey\",\"box\",\"office\",\"million\"],\"marker\":{\"size\":[7,7,6,4,2,2,4,1,1,1],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"fit\",\"opacity\":0.6,\"text\":[\"remake\",\"ariel\",\"new\",\"thelittlemermaid\",\"woke\",\"race\",\"hallebailey\",\"box\",\"office\",\"million\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[529,195,193,160,152,151,98,82,79,73],\"y\":[0.025521098168015382,0.009407892769030226,0.009311406509156062,0.00771938322123238,0.007333438181735729,0.007285195051798648,0.004728309165133339,0.003956419086140039,0.0038116896963287946,0.0035222309167063067],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"ariel\",\"black\",\"race\",\"remake\",\"white\",\"woke\",\"people\",\"story\",\"original\",\"new\"],\"marker\":{\"size\":[7,3,2,7,1,2,2,1,1,6],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"social context independency\",\"opacity\":0.6,\"text\":[\"ariel\",\"black\",\"race\",\"remake\",\"white\",\"woke\",\"people\",\"story\",\"original\",\"new\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[1995,1706,1616,1445,1052,1040,906,645,617,600],\"y\":[0.020998924902537058,0.017956990628105747,0.01700967545613752,0.015209776629397884,0.01107316704513662,0.010946858355540857,0.009536411321721495,0.006789197323013633,0.006494477047290184,0.006315539737029518],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"ariel\",\"hallebailey\",\"remake\",\"thelittlemermaid\",\"best\",\"new\",\"one\",\"perfect\",\"favorite\",\"love\"],\"marker\":{\"size\":[7,4,7,4,1,6,2,1,1,1],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"conformity\",\"opacity\":0.6,\"text\":[\"ariel\",\"hallebailey\",\"remake\",\"thelittlemermaid\",\"best\",\"new\",\"one\",\"perfect\",\"favorite\",\"love\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[1597,998,965,930,451,302,293,247,227,213],\"y\":[0.031780446053776876,0.01986036591263039,0.019203667006690767,0.018507168167057834,0.008975084047509978,0.006009988987358347,0.00583088928573845,0.004915490810792309,0.004517491473859205,0.004238891938006031],\"type\":\"scatter\"},{\"hoverlabel\":{\"font\":{\"color\":\"#FFFFFF\"}},\"hovertext\":[\"ariel\",\"thelittlemermaid\",\"remake\",\"hallebailey\",\"world\",\"new\",\"ursula\",\"play\",\"review\",\"starring\"],\"marker\":{\"size\":[7,4,7,4,1,6,2,2,1,1],\"sizemin\":4,\"sizemode\":\"area\",\"sizeref\":0.00875},\"mode\":\"markers+text\",\"name\":\"historical value\",\"opacity\":0.6,\"text\":[\"ariel\",\"thelittlemermaid\",\"remake\",\"hallebailey\",\"world\",\"new\",\"ursula\",\"play\",\"review\",\"starring\"],\"textposition\":\"bottom center\",\"visible\":\"legendonly\",\"x\":[855,780,770,702,373,311,164,161,145,126],\"y\":[0.01924729764348299,0.017558957947735307,0.017333845988302284,0.01580308466415772,0.00839690119881123,0.007001207050326481,0.003692061246661027,0.0036245276588311197,0.0032643485237382813,0.002836635800815536],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tweet Count\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Word Importance\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"   <b>Topic</b>\"},\"itemsizing\":\"constant\"},\"margin\":{\"t\":15,\"b\":0,\"l\":20,\"r\":0}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b89a06cf-ccce-4911-bddb-67d68418035e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2de3Rx-HpGdz",
        "outputId": "388903c9-6980-4eb5-8428-2d187da2565e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Convert 'timestamp' to pandas datetime format\n",
        "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
        "\n",
        "# Step 2: Filter data for the year 2023 and months up to June (1 to 6)\n",
        "tweets_df_2023 = tweets_df[(tweets_df['timestamp'].dt.year == 2023) & (tweets_df['timestamp'].dt.month <= 6)]\n",
        "\n",
        "# Step 3: Group data by clusters\n",
        "cluster_groups = tweets_df_2023.groupby('cluster')\n",
        "\n",
        "# Step 4: Create a mapping dictionary for cluster numbers to cluster names\n",
        "cluster_name_map = dict(zip(topics_df['cluster'], topics_df['topic_name']))\n",
        "\n",
        "# Step 5: Time Series Analysis and Step 6: Visualization\n",
        "for cluster, cluster_data in cluster_groups:\n",
        "    # Resample data to daily frequency and count the number of tweets per day\n",
        "    cluster_data_resampled = cluster_data.resample('D', on='timestamp').size()\n",
        "\n",
        "    # Calculate moving average for smoother trend visualization\n",
        "    moving_avg = cluster_data_resampled.rolling(window=7).mean()\n",
        "\n",
        "    # Plot time trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(cluster_data_resampled.index, cluster_data_resampled, label=cluster_name_map[cluster])\n",
        "    plt.plot(cluster_data_resampled.index, moving_avg, label=f'Moving Average ({cluster_name_map[cluster]})', color='red')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Tweets')\n",
        "    plt.title(f'Time Trends for {cluster_name_map[cluster]} in 2023 (Up to June)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot in the current working directory\n",
        "    file_name = f'time_trends_{cluster_name_map[cluster]}.png'\n",
        "    plt.savefig(file_name)\n",
        "\n",
        "    plt.close()  # Close the current figure to avoid overlapping of plots\n"
      ],
      "metadata": {
        "id": "m1uSKhWJqfDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset from the CSV file\n",
        "datacsv = pd.read_csv('/content/disney7topics_results.csv')\n",
        "\n",
        "# Calculate the total number of tweets in the 'tweets' column\n",
        "total_tweets = datacsv['filtered_tweets'].count()\n",
        "\n",
        "print(\"Total Number of Tweets:\", total_tweets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m7j_OQVwKre",
        "outputId": "bf3694e5-0d32-4e34-ad97-4485c7ad4ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Tweets: 19670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calculate the total sum of values for each row and save it in a new column\n",
        "data['total_num_topic_occurrence'] = data['num_topic_occurrence'].apply(sum)\n",
        "\n"
      ],
      "metadata": {
        "id": "O-t3fREfCgz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_topics = 228\n",
        "\n",
        "# Calculate the HHI for each topic\n",
        "data['topic_hhi'] = (data['total_num_topic_occurrence'] / total_topics) ** 2\n",
        "\n",
        "# Calculate the overall HHI\n",
        "overall_hhi = data['topic_hhi'].sum()\n",
        "\n",
        "print(\"HHI for Each Topic:\")\n",
        "print(data['topic_hhi'])\n",
        "print(\"Overall HHI:\", overall_hhi)"
      ],
      "metadata": {
        "id": "gGPJnfhjE8Uy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}